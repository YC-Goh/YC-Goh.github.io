#   Wish it Rained Overnight

I miss Autumn rains.

##  Speculation

### [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://machinelearning.apple.com/research/illusion-of-thinking?utm_source=alphasignal)

1.  Not a surprise since the underlying engines of RAGs and Reasoning Models are LLMs,
    which are fundamentally next-word prediction machines.
2.  What is most likely happening is that the "break down the problem to smaller steps"
    and "answer this smaller step"
    and "infer the solution to the next step given this step"
    steps are all sufficiently represented by the training data.
5.  Still, despite being not "true" reasoning
    (like, do most humans actually do better than this?),
    this is already sufficiently useful for many real-world needs
    especially in the vast majority of cognitive work that deals with in-sample problems
    that are often farmed out to lower-ranked / lower-waged workers.
    Some reminders:
    -   "Computer" was once a semi-skilled profession,
        until personal computers came along.
    -   "Typist" was once a semi-skilled profession,
        until word processor software came along.

##  New Outstanding

-   Maybe (just, maybe) add KaTeX integration.
    -   If so, then convert all usage of math to TeX.